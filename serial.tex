%------------------------------------------------------------------------------
% Template file for the submission of papers to IUCr journals in LaTeX2e
% using the iucr document class
% Copyright 1999-2013 International Union of Crystallography
% Version 1.6 (28 March 2013)
%------------------------------------------------------------------------------

\documentclass[preprint]{iucr}              % DO NOT DELETE THIS LINE

     %-------------------------------------------------------------------------
     % Information about journal to which submitted
     %-------------------------------------------------------------------------
     \journalcode{J}              % Indicate the journal to which submitted
                                  %   A - Acta Crystallographica Section A
                                  %   B - Acta Crystallographica Section B
                                  %   C - Acta Crystallographica Section C
                                  %   D - Acta Crystallographica Section D
                                  %   E - Acta Crystallographica Section E
                                  %   F - Acta Crystallographica Section F
                                  %   J - Journal of Applied Crystallography
                                  %   M - IUCrJ
                                  %   S - Journal of Synchrotron Radiation
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\begin{document}                  % DO NOT DELETE THIS LINE

     %-------------------------------------------------------------------------
     % The introductory (header) part of the paper
     %-------------------------------------------------------------------------

     % The title of the paper. Use \shorttitle to indicate an abbreviated title
     % for use in running heads (you will need to uncomment it).

\title{Application of signal separation to diffraction image compression and serial crystallography}
%\shorttitle{Short Title}

     % Authors' names and addresses. Use \cauthor for the main (contact) author.
     % Use \author for all other authors. Use \aff for authors' affiliations.
     % Use lower-case letters in square brackets to link authors to their
     % affiliations; if there is only one affiliation address, remove the [a].

\cauthor[a]{Jérôme}{Kieffer}{jerome.kieffer@esrf.fr}{}
\author[a]{Nicolas}{Coquelle}
\author[a]{Samuel}{Debionne}
\author[a]{Alejandro}{Homs}
\author[a]{Gianluca}{Santoni}
\author[a]{Götz}{Andrew}
\author[a]{Daniele}{De Sanctis}

\aff[a]{European Synchrotron Radiation Facility;71, avenue des Martyrs;CS 40220;38043 Grenoble Cedex 9 \country{France}}

     % Use \shortauthor to indicate an abbreviated author list for use in
     % running heads (you will need to uncomment it).

%\shortauthor{Soape, Author and Doe}

     % Use \vita if required to give biographical details (for authors of
     % invited review papers only). Uncomment it.

%\vita{Author's biography}

     % Keywords (required for Journal of Synchrotron Radiation only)
     % Use the \keyword macro for each word or phrase, e.g. 
     % \keyword{X-ray diffraction}\keyword{muscle}

%\keyword{keyword}

     % PDB and NDB reference codes for structures referenced in the article and
     % deposited with the Protein Data Bank and Nucleic Acids Database (Acta
     % Crystallographica Section D). Repeat for each separate structure e.g
     % \PDBref[dethiobiotin synthetase]{1byi} \NDBref[d(G$_4$CGC$_4$)]{ad0002}

%\PDBref[optional name]{refcode}
%\NDBref[optional name]{refcode}

\maketitle                        % DO NOT DELETE THIS LINE

\begin{synopsis}
Precise background assessment and application to single crystal image compression and serial crystallography data reprocessing. 
\end{synopsis}

\begin{abstract}
Abstract goes here.
\end{abstract}


     %-------------------------------------------------------------------------
     % The main body of the paper
     %-------------------------------------------------------------------------
     % Now enter the text of the document in multiple \section's, \subsection's
     % and \subsubsection's as required.

\section{Introduction}

%\cite{Cheetah2014}
The Synchrotron serial crystallography beamline (ID29 at ESRF) has been built to \ldots
In this setup samples will be carried in front of the beam and a Jungfrau 4M detector records 


\section{Algorithm for the separation of amorphous background from Bragg peaks}
\subsection{Background scattering}

The simplest implementation of Bragg peaks separation is to consider that the background originates from scattering from amorphous material, giving isotropic signal ideally presenting only smooth variations.
For this the raw signal has to be corrected from dark noise and any systematic anisotropic effects like polarization corrections.

The initial implementation in pyFAI \cite{pdj2013} relied on a 2D radial transform followed by a median filter in the azimuthal dimension 
to separate amorphous scattering from crystalline scattering.
Despite this method has been successfully used for large dataset analysis \cite{brocades}, it presents several major drawbacks:
* The 2D averaging mixes the signal of several pixels and blurs the signal. 
* Pixel-splitting is needed to leverage the moiré effect in the 2D averaging, but this increases further the blurring. 
* The 1D curve obtained after the application of the median filter shows sharp jumps from one azimuthal bin to its neighbour.
* Median filter is complicated and requires a lot of memory.
%Conceptually complicated ot computationally heavy?
    
The coming sections present an efficient way to perform the azimuthal averaging and the associated variance propagation, 
and its application to the statistical analysis and extraction of background from a diffraction image. 

\subsection{Efficient azimuthal averaging and uncertainties evaluation}

\subsubsection{Preprocessing}
The first step of the analysis consists in applying a pixel-wise correction for dark current and several normalization corrections\cite{pyfai_2020} which is described in Equation\ref{norm}:
\begin{equation}
\label{norm}
I_{cor} = \frac{signal}{norm}  = \frac{I_{raw} - I_{dark}}{F \cdot
\Omega \cdot P \cdot A \cdot I_0} 
\end{equation}
Here the numerator ($signal$ hereafter) is the subtraction of the dark current $I_{dark}$ from the the detector's raw signal $I_{raw}$.
The denominator (hereafter $norm$) is a normalization factor composed of the product of  $F$:  a factor accounting for the flat-field correction, $\Omega$: the solid
angle subtended by a given pixel, $P$: the polarisation correction term and
$A$: the detector's apparent efficiency due to the incidence angle of the
photon on the detector plane (for integrating detectors, high energy photons with
larger incidence angle see larger sensor thickness, and thus have higher
detection probability).
Intensity can also be normalized by the incoming flux $I_0$, which is independent of the pixel position.
%Normalizing By I0 should not change much though, am I right?

\subsubsection{Azimuthal averaging} 
%This is my understanding: before you had a
Historically, the azimutal averaging was implemented using histograms. Since the the geometry of the experimental setup is fixed during the acquisition, a look-up table listing all pixels contributing to each  azimuthal bin can be built and used to speed-up calculations \cite{pyFAI_gpu}.
The azimuthal transformation is a linear transformation, and it can thus be implemented as a  matrix multiplication between a sparse-matrix representing the transformation and a dense vector which is simply the flattened view of the diffraction image. 
The compressed sparse row (CRS) matrix representation is preferred since it is very efficient to perform dot products with dense vectors.
The coefficients $c_{i,r}$ of the matrix are the fraction of area of a pixel $i$ falling into the radial bin $r$.
In the case where pixel splitting is deactivated those coefficients  ($c_{i,r}$) are always one (and zero elsewhere) since each pixel contribute to a single bin.
The sparse matrix multiplication can be used to sum efficiently values for all pixels belonging to the same bin.
The summed signal divided by the summed normalization provides the intensity averaged over all pixels falling in the bin at the distance $r$ from the center,as described by equation \ref{avg}: 
\begin{equation}
\label{avg}
<I>_{r} = \frac{\sum\limits_{i \in bin_r} c_{i,r} \cdot signal_i}
                        {\sum\limits_{i \in bin_r} c_{i,r} \cdot norm_i} 
\end{equation}  

\subsubsection{Uncertainty evaluation from Poisson law}
Photon counting detectors suffer from several uncertainties, among which the most important one is usually the counting statistics (often referred as Poisson law),
stating that the lower limit for the variance for a pixel is the number of events counted.
Other sources of noise, like the dark current noise,  superimpose quadratically to the Poisson noise as presented in equation \ref{poisson}:     
\begin{equation}
\label{poisson}
var(I) = (\sigma(I))^{2} = I_{raw} + (\sigma_{dark})^{2}  
\end{equation}
%Jerome, I don't think the regrouping has been introduced before. MAybe worth doing it.
During the regrouping part, the coefficients of the sparse matrix needs to be squared for the numerator to have uncertainties $\sigma$ proportional to the raw signal, as shown in equation \ref{varianceP}.
\begin{equation}
\label{varianceP}
(\sigma_{r}(I))^2 = \frac{\sum\limits_{i \in bin_r} c_{i,r}^2 \cdot var_i}
                  {\sum\limits_{i \in bin_r} c_{i,r} \cdot norm_i} 
\end{equation}
One should distinguish the \textit{uncertainty of the mean} (sometimes referred are standard error of the mean or $sem$), 
which describes the precision with which the mean is known (and described in \cite{pyfai_2020}),
from the \textit{uncertainty of the pixel value} (often referred are standard deviation, $std$). 
Those two value differ only by square root of the number of measurements: $sem = std/\sqrt{norm}$.
Thus, the more data point are collected, the more precisely the mean value is known.
Since this document focuses on the uncertainties of pixel values, the \textit{standard deviation} will systematically be used in this document.  

\subsubsection{Uncertainty evaluation from variance in a bin.}
When considering the diffraction of an isotropic compound (liquid, amorphous or perfect powder), all pixels falling into the same radial bin should see the same flux of photons and the deviation to their intensities could be used to estimate the uncertainty.
Variance (and standard deviation) are usually obtained in a two steps procedure: one pass to calculate the average value (Eq\ref{avg})and a second to calculate the deviation to the average (Eq\ref{varA}. 
This double pass approach can be implemented using sparse matrix multiplication. It requires the access to each pixel twice and extra storage space, but it is very stable numerically.
\begin{equation}
\label{varA}
(\sigma_{r}(I))^2 = \frac{\sum\limits_{i \in bin_r} c_{i,r} \cdot norm_i \cdot (\frac{signal_i}{norm_i}-<I>_r)^2}
                         {\sum\limits_{i \in bin_r} c_{i,r} \cdot norm_i}  ?=  \sum\limits_{i \in bin_r} c_{i,r} \cdot  (\frac{signal_i}{norm_i}-<I>_r)^2
\end{equation}


 
Single pass implementations of variance calculation are faster since they access pixels only once and offer the ability to perform parallel reductions, i.e. work with blocks of pixels.
\citeasnoun{variance2018} presents a complete review, from which we adapt to the formalism of crystallography some key equations.
For example the weight for a pixel is $\omega_i = c_i \cdot normalization_i$
If $P$ is a partition of the ensemble of pixels falling into a given azimuthal bin, let $\Omega_{P}$, $V_{P}$ and $VV_{P}$  
be the weight sum, the weighted sum of $V$ and the weighted sum of deviation squared over the partition $P$: 
\begin{equation}
\Omega_{P} = \sum\limits_{i \in P} \omega_i = \sum\limits_{i \in P} c_i \cdot norm_i 
\end{equation}
\begin{equation}
V_{P} = \sum\limits_{i \in P} \omega_i \cdot v_i =  \sum\limits_{i \in P} c_i \cdot signal_i
\end{equation}
\begin{equation}
VV_{p} = \sum\limits_{i \in P} \omega_i \cdot (v_i - V_{P}/\Omega_{P})^2 
\end{equation}

The average and the variance are then expressed as:
\begin{equation}
<I>_P = V_{P}/\Omega_{P} =  \frac{\sum\limits_{i \in P} c_i \cdot signal_i}
                        {\sum\limits_{i \in P} c_i \cdot norm_i} 
\end{equation}

\begin{equation}
(\sigma_P(I))^2 = VV_{P}/\Omega_{P} 
\end{equation}

Performing the union of two partition $A$ and $B$ allows the parallel reduction, which is especially efficient on GPU:
\begin{equation}
\Omega_{A \cup B} =  \Omega_{A} + \Omega_{B} 
\end{equation}

\begin{equation}
V_{A \cup B} =  V_{A} + V_{B} 
\end{equation}
  
\begin{equation}
VV_{A \cup B} =  VV_{A} + VV_{B} +  \frac{(\Omega_{A} \cdot V_{B} - \Omega_{B}\cdot V_{A})^2}{V_{A \cup B} \cdot  V_{A} \cdot V_{B}}
\end{equation}
  
The numerical stability issue is addressed by using double-precision arithmetic when implemented on CPU and double-word arithmetic when running on GPU \cite{double_word}.

\subsection{Histogram intensity }

The figure \ref{fig1}a presents the diffraction from a single crystal of insulin and several curves obtained from azimuthal integration: 
\ref{fig1}b is the azimuthally integrated signal (blue curve). Bragg peaks are seen as spikes on top of the background.
The plot \ref{fig1}c presents the uncertainties measured according to the Poisson law (orange curve, valid since the Pilatus is a photon counting detector) 
or the deviation in the ring (blue curve), much larger value since Bragg peaks contribute a lot to the deviation despite they represent few pixels.         
In the plot \ref{fig1}d are presented histogram of pixel intensity for  pixel laying at 80mm and 160mm from the beam center. 
Each of those histograms is composed of a bell-shaped distribution, centered on the average value with negative outliers tagged with the pixel value -1
(this is specific to the Pilatus detector), and few positive outliers which are usually Bragg peaks.   
Those histograms in figure \ref{fig1}d have been fitted with a Gaussian curve and both the center ($\mu$) and the width ($\sigma$) of the curve match 
roughly with the average (in \ref{fig1}b) and uncertainties (in \ref{fig1}c).  
\begin{figure}
\label{fig1}
\begin{center}
\includegraphics[width=14cm]{fig1}
\caption{Single crystal diffraction frame obtained from insulin with a Pilatus6M (a) (part of Dectris's reference dataset) with the azimuthally averaged signal (b), 
before and after clipping data. Uncertainties are presented in (c) when calculated assuming a Poissonian error model or when measuring the deviation within all pixels in a ring.
Subplot (d) presents the histogram of intensities for two rings at r=80mm and r=160mm from beam center with the distribution fitted as Gaussians.}
\end{center}
\end{figure}

The core idea of the algorithm for background extraction is to model the distribution of background pixels. Unlike Bayesian statistics \cite{Sivia2006} where the cost function is usually tuned to weight less outlier, here, those outliers are simply flagged and discarded.
Positive outliers can reasonably be assigned to Bragg peaks and negative outliers to shadows or defective pixels. 
The distribution is recalculated  after discarding pixels for which intensity differ from the average value by more than $n$ times the standard deviation (Eq\ref{clip}) which has the effect of re-centering the distribution and enforcing a normal distribution of the remaining values:
\begin{equation}
\label{clip}
|I - <I>| > n \cdot \sigma(I)
\end{equation}
The orange plot in figure \ref{fig1}b presents the average after having discarded those outliers, and the orange and green curve of figure \ref{fig1}c are the uncertainties calculated after this clipping. 
 After clipping the average and uncertainties curves have lost most of their spikes, which means that Bragg peaks and shadowed pixel were discarded.
 
\subsection{Sigma-clipping}
The \textit{sigma-clipping} algorithm consists in iteratively applying the outlier rejection previously described in order to enforce the background distribution to follow a normal law.
There are two parameters to this algorithm: the number of iterations of clipping and the rejection cut-off.
%I merged number of iterations and sigma clipping  as closely connected.
Despite the execution time is proportional to the number of iteration of sigma-clipping, iterations should continue until no more outliers are found and the 
background distribution actually matches a Gaussian distribution. 
Since the loop exits as soon as no more outliers were discarded at the clipping step, having an arbitrary large number of iteration is not an issue for the execution time and the number of actual iteration is usually few (3 is commonly observed).       

\subsubsection{Clipping threshold:}
Clipping threshold can be automatically calculated based on a variation on Chauvenet's criterion \cite{chauvenet} where one would accept to discard only a single pixel in a ring with a signal already following a normal law. 
Thus, the threshold value is adapted to the $size$ of the distribution, i.e. the number of pixels in each ring (Eq\ref{chauvenet}), which can reach several thousands and shrinks with iterations.
Typically the numerical value for this cut-off varies from 2 to 4.   
\begin{equation}
\label{chauvenet}
SNR_{chauvenet} =  \sqrt{2 log(\frac{size}{\sqrt{2 \pi}})}
\end{equation}
% could be interesting to show Threshold vs cycle for the example above (if relecant)

\section{Application to single crystal diffraction image compression}
%I explicit here MAcromolecular as we are talking about id29. Remove is superfluous.
Diffraction images from a single macromolecular crystal exhibit usually an isotropic background on top of which Bragg peaks appear.
The sigma-clipping algorithm can be used to select the background level and more importantly the associated uncertainty.

This lossy compression algorithm consists in picking (and storing) pixels which intensity is above the background value plus $p$ standard deviation. This value $p$ controls the amount of data to store and provides also an upper level for the compression ratio obtained since with $p=1$, 16\% of the pixel are stored but only 2.4\% for $p=2$ and 0.15\% for $p=3$.
%derived from equation ?? 

For each stored pixels we need to record not only its intensity but also its position. 
Moreover, to be able to regenerate the background, the averaged and uncertainties curves needs to be stored.
The following section presents 

\subsection{Sparsification}

The sigma-clipping algorithm was originally written for real-time sparsification of synchrotron serial crystallography data and its integration into the LIMA2 \cite{lima2} detector control system for the Jungfrau 4M detector used at ESRF ID29 is described in \cite{sri2021}.
The real-time constrain imposed to develop code running on GPU since those devices are several times
faster than equivalent processors (even when running on all cores).
%What do you mean by equivalent? How s a processor equivalent to a GPU?
All algorithms were developed in OpenCL \cite{opencl_khronos} and implemented in the \textit{pyFAI} software package.
A command line tool called `sparsify-Bragg' is available for off-line testing.

All the pixel coordinates and intensities are finally stored in a HDF5 \cite{hdf5} container following the nexus \cite{nexus} convention, together with a snippet of Python code explaining how to rebuild the dataset.
All sparse datasets (averaged and uncertainties curves, pixel corrdinates, ...) are compressed with the bitshuffle-LZ4 \cite{bitshuffle} lossless compression.

\subsection{Densification}
Since no crystallographic software package is aware of this sparse format, the densification code was developed to regenerate initial frames and the `densify-Bragg` program was made available as part of the FabIO \cite{fabio} software package (version $\ge$0.12). 
The software constrains for this densification code are very different from the one for sparisfication. For this reason `densify-Bragg' was optimized to run on multicore CPU. 
The only important options are, beside the file-format and name, whether the background noise has to be reconstructed or not. 
Some crystallographic reduction program like CrysAlisPro \cite{crysalis} provide a better result with an almost constant background while XDS \cite{xds}, which performs a deep noise analysis, needs to have the noisy background restored. 
Shaded regions are never reconstructed properly and should be masked adequately in the reduction software.

\subsection{Performances}
The performances for a lossy compression algorithm are to be evaluated along many directions: compression and decompression speeds, compression ratio and degradation of the recorded signal.
All those factors have been evaluated on a classical sample, Lysozyme, and rotational data were collected on an Eiger 4M detector (selected for its similarity in size and performances with the Jungfrau detector). 
This reference dataset is the one made publicly available by Dectris to \cite{lysozym}. 

\subsubsection{Compression ratio} 
After sparsification (picking cut-off: $2\sigma$, error-model: Poisonnian), the dataset still weights 103MB which represents a 15x compression ratio compared to the standard procedure. 
The maximum achievable compression ratio for $2\sigma$ is 42x. 
In production condition, one would likely select a lower threshold to collect more pixels.

For conformance with the state of the art, the reference the dataset size is the one re-compressed using the bitshuffle-LZ4 \cite{bitshuffle}, for  which the 1800 frames weight 1500MB (instead of the 5000GB of the original files compressed in LZ4).

\subsubsection{Compression speed:}
%\begin{verbatim}
%kieffer@scisoft14:~/Dectris/EIGER_X_4M_lyso/demo$ sparsify-Bragg ../collect_01_00001_master.h5 -p ../geometry.poni  --bins 500 --unit="r_mm" --cycle=5 --cutoff-clip=0  --noise=1 --profile -m ../mask.npy --cutoff-pick=2.0 --error-model=poisson -o sparse.h5
%INFO:pyFAI.method_registry:Degrading method from Method(dim=1, split='pseudo', algo='histogram', impl='*', target=None) -> Method(dim=1, split='bbox', algo='histogram', impl='*', target=None)
%INFO:pyFAI.geometry:Unable to parse ../geometry.poni as JSON file, defaulting to PoniParser
%WARNING:pyFAI.ext.splitBBoxCSR:Pixel splitting desactivated !
%INFO:silx.opencl.processing:379.313MB are needed on device: TITAN V,  which has 12652.839MB
%INFO:silx.opencl.processing:Compiling file ['silx:opencl/doubleword.cl', 'pyfai:openCL/preprocess.cl', 'pyfai:openCL/memset.cl', 'pyfai:openCL/ocl_azim_CSR.cl', 'pyfai:openCL/peak_finder.cl', 'pyfai:openCL/peakfinder8.cl'] with options -D NBINS=500  -D NIMAGE=4485690 -D WORKGROUP_SIZE=1024
%INFO:silx.opencl.processing:■■■■■■■■■■■■■■■■■■■]  100%  Saving: sparse.h5                      
%OpenCL kernel profiling statistics in milliseconds for: OCL_PeakFinder
%                                       Kernel name (count):      min   median      max     mean      std
%                                 copy H->D indices (    1):    3.730    3.730    3.730    3.730    0.000
%                                  copy H->D indptr (    1):    0.001    0.001    0.001    0.001    0.000
%                                    copy H->D mask (    1):    1.096    1.096    1.096    1.096    0.000
%                                copy H->D radius1d (    1):    0.002    0.002    0.002    0.002    0.000
%                                copy H->D radius2d (    1):    3.489    3.489    3.489    3.489    0.000
%                               copy raw H->D image ( 1800):    3.615    5.064    5.718    4.893    0.412
%                                 cast u32_to_float ( 1800):    0.070    0.075    0.103    0.074    0.001
%                                         memset_ng ( 1800):    0.004    0.004    0.010    0.004    0.000
%                                       corrections ( 1800):    0.169    0.171    0.178    0.172    0.001
%                                   csr_sigma_clip4 ( 1800):    3.077    3.230    4.140    3.267    0.133
%                                    memset counter ( 1800):    0.004    0.004    0.015    0.004    0.001
%                                       peak_search ( 1800):    0.261    0.263    0.272    0.263    0.001
%                                 copy D->H counter ( 1800):    0.001    0.001    0.010    0.001    0.000
%                  copy D->D + cast uint32 intenity ( 1800):    0.007    0.010    0.025    0.010    0.002
%                           copy D->H peak_position ( 1800):    0.003    0.006    0.029    0.009    0.005
%                           copy D->H peak_intensty ( 1800):    0.003    0.006    0.029    0.009    0.005
%                          copy D->H background_avg ( 1800):    0.002    0.002    0.004    0.002    0.000
%                          copy D->H background_std ( 1800):    0.002    0.002    0.003    0.002    0.000
%________________________________________________________________________________
%                       Total OpenCL execution time        : 15687.401ms
%\end{verbatim}

The compression speed has been measured on the computer designed for online data-reduction of the Jungfrau detector \cite{sri2021}: 
an IBM AC922 using Power9 processors and Nvidia Tesla V100 GPU. 
The code is and serial takes about 9 ms to process one image and uses a single CPU-core and about 25\% of the GPU. 
In online condition, multiple parallel processes are expected to achieve much higher frame-rate, since the nominal speed for the Jungfrau is 1kHz.

\subsubsection{Decompression speed:} 
The decompression of those data should typically be performed on a standard workstation (here running two Intel Xeon Gold 6134 CPU @ 3.20GHz): the reconstruction speed is found to take 30s while the writing of the densified dataset (with bitshuffle-LZ4 compression) takes 45s. 
%maybe explicit that the decompression is for the full dataset and not 30s per frame
Densification is thus faster than writing on disk.
The reading of the input sparse dataset is negligible (<2s).

\subsubsection{Quality of the restored dataset:} 
The densified dataset was processed via XDS and the summary indicator for the quality of the results are compared with the one coming from the reduction of the original dataset. 
Since those integrator are measured on integral peaks with $I/\sigma>3$ and the sparsification was performed
with a cut-off of 2, those result should be almost unaffected, which is confirmed in the table \ref{} hereafter:
\begin{verbatim}
 R-FACTOR
 observed = (SUM(ABS(I(h,i)-I(h))))/(SUM(I(h,i)))
 expected = expected R-FACTOR derived from Sigma(I)

 COMPARED = number of reflections used for calculating R-FACTOR
 I/SIGMA  = mean of intensity/Sigma(I) of unique reflections
            (after merging symmetry-related observations)
 Sigma(I) = standard deviation of reflection intensity I
            estimated from sample statistics

 R-meas   = redundancy independent R-factor (intensities)
            Diederichs & Karplus (1997), Nature Struct. Biol. 4, 269-275.

 CC(1/2)  = percentage of correlation between intensities from
            random half-datasets. Correlation significant at
            the 0.1% level is marked by an asterisk.
            Karplus & Diederichs (2012), Science 336, 1030-33
 Anomal   = percentage of correlation between random half-sets
  Corr      of anomalous intensity differences. Correlation
            significant at the 0.1% level is marked.
 SigAno   = mean anomalous difference in units of its estimated
            standard deviation (|F(+)-F(-)|/Sigma). F(+), F(-)
            are structure factor estimates obtained from the
            merged intensity observations in each parity class.
  Nano    = Number of unique reflections used to calculate
            Anomal_Corr & SigAno. At least two observations
            for each (+ and -) parity are required.

reference: total      119561   13801     14844       93.0\%      12.5\%     15.0\%   119049   10.52     13.2\%    99.7*   -18    0.630   10151
sparse:    total      120588   13802     14837       93.0\%      11.4\%     12.1\%   120131   10.17     12.0\%    99.7*   -12    0.631   10176
\end{verbatim}
\section{Application to serial crystallography}

\subsection{Performance}
     % comparison with cheetah
\section{Conclusion}
drawback: textured background

     %-------------------------------------------------------------------------
     % The back matter of the paper - acknowledgements and references
     %-------------------------------------------------------------------------

     % Acknowledgements come after the appendices

\ack{Acknowledgements}
The authors would like to thank Gavin Vaughan, scientist at Materials beamline at the ESRF,  for the constructive discussion about sigma-clipping versus median filtering. 
We would like to thank also Jonathan P. Wright and Carlotta Giacobbe for offering us the ability to test those algorithms on small molecule data and validate the concept on the ID11 beamline from the ESRF.


\bibliographystyle{iucr}
\bibliography{biblio}

\end{document}                    % DO NOT DELETE THIS LINE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
